(self.webpackChunkguzzle=self.webpackChunkguzzle||[]).push([[7466],{3905:function(e,t,i){"use strict";i.d(t,{Zo:function(){return u},kt:function(){return h}});var a=i(7294);function n(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function r(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,a)}return i}function l(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?r(Object(i),!0).forEach((function(t){n(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):r(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function o(e,t){if(null==e)return{};var i,a,n=function(e,t){if(null==e)return{};var i,a,n={},r=Object.keys(e);for(a=0;a<r.length;a++)i=r[a],t.indexOf(i)>=0||(n[i]=e[i]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)i=r[a],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(n[i]=e[i])}return n}var p=a.createContext({}),s=function(e){var t=a.useContext(p),i=t;return e&&(i="function"==typeof e?e(t):l(l({},t),e)),i},u=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var i=e.components,n=e.mdxType,r=e.originalType,p=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),d=s(i),h=n,m=d["".concat(p,".").concat(h)]||d[h]||c[h]||r;return i?a.createElement(m,l(l({ref:t},u),{},{components:i})):a.createElement(m,l({ref:t},u))}));function h(e,t){var i=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=i.length,l=new Array(r);l[0]=d;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o.mdxType="string"==typeof e?e:n,l[1]=o;for(var s=2;s<r;s++)l[s]=i[s];return a.createElement.apply(null,l)}return a.createElement.apply(null,i)}d.displayName="MDXCreateElement"},8383:function(e,t,i){"use strict";i.r(t),i.d(t,{frontMatter:function(){return o},metadata:function(){return p},toc:function(){return s},default:function(){return c}});var a=i(2122),n=i(9756),r=(i(7294),i(3905)),l=["components"],o={},p={unversionedId:"how_to_guides/ingest_data/legacy/pipeline_lagacy",id:"how_to_guides/ingest_data/legacy/pipeline_lagacy",isDocsHomePage:!1,title:"pipeline_lagacy",description:"Introduction",source:"@site/docs/how_to_guides/ingest_data/legacy/pipeline_lagacy.md",sourceDirName:"how_to_guides/ingest_data/legacy",slug:"/how_to_guides/ingest_data/legacy/pipeline_lagacy",permalink:"/docs/how_to_guides/ingest_data/legacy/pipeline_lagacy",editUrl:"https://github.com/ja-guzzle/docs/blob/master/docs/how_to_guides/ingest_data/legacy/pipeline_lagacy.md",version:"current",frontMatter:{}},s=[{value:"Introduction",id:"introduction",children:[]},{value:"Usage",id:"usage",children:[]},{value:"Steps to create pipeline",id:"steps-to-create-pipeline",children:[]},{value:"Pipeline Configurations",id:"pipeline-configurations",children:[]},{value:"Settings when adding Activtiy to pipeline",id:"settings-when-adding-activtiy-to-pipeline",children:[]},{value:"Dependency graph (lineage)",id:"dependency-graph-lineage",children:[]},{value:"Pipeline UI Features",id:"pipeline-ui-features",children:[]},{value:"Pipeline parameters",id:"pipeline-parameters",children:[{value:"How to define a pipeline parameter",id:"how-to-define-a-pipeline-parameter",children:[]}]},{value:"Dynamic Activity support in the pipeline",id:"dynamic-activity-support-in-the-pipeline",children:[{value:"Supported datastores:",id:"supported-datastores",children:[]},{value:"Placeholder format",id:"placeholder-format",children:[]}]},{value:"Pipeline resume and Partial run",id:"pipeline-resume-and-partial-run",children:[]}],u={toc:s};function c(e){var t=e.components,i=(0,n.Z)(e,l);return(0,r.kt)("wrapper",(0,a.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"A Guzzle Pipeline is a logical grouping of activities that together perform a task. For example, a Pipeline could contain a set of activities that ingest and clean log data, and then kick off a mapping data flow to analyze the log data.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The Pipeline allows you to manage the activities as a set instead of each one individually. You deploy and schedule the Pipeline instead of the activities independently.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The activities in a Pipeline define actions to perform on your data. For example, you may use a Guzzle Ingestion activity to copy data from an Azure Blob Storage to Delta table.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Then, use a Processing activity to process and transform data from delta table on top of which business intelligence reporting solutions are built.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Guzzle Pipeline can have multiple groupings of activities: Ingestion, processing, contraint check, reconsiliation and external activity.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"You can do auto-dependency or run sequentially/parallel if its all independent.\n-You can put a cap of how many jobs to run parallel  - this helps to throttle (we do task allocation to parallel execution threads dynamically - so that all threads will have something to run till we have exhausted - not like ADF which does static allocation of jobs and you get into situation where 2 out of 5 threads are still running and have more that few jobs to run because they ended up getting long running jobs)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Param passed to pipeline are auto passed to activities - they can also be pre-processed /manipulated before passing to individual activities added in pipe (we let you do that thru groovy template engine - similar to jinja)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"You can override spark setting - including stating that run a particular activity in job cluster while rest can run on default compute which was used at time of running pipe (all-purpose cluster); or overwrite spark confs or size of cluster. ")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"You can rerun pipeline from the point it failed without need of passing any execution id of previous run- Just submit  pipeline again with the same set of business date or other param you had passed for the first run - and it will detect the latest failed pipeline run if it exists - and it will run from there  (you can also decide to run whole thing again by putting resume=true)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"We support multitask feature of databricks - which means guzzle can submit entire pipeline as single workflow in databricks and even support advance feature of databricks like reusing job cluster across multiple jobs")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Dynamic pipeline helps you to fetch the list of activties along with their param to run dynamically - this is useful if you want to skip some silver jobs if there was no data loaded in bronze layers for those tables; or skip ingestion jobs if no new data shows up in files or source tables ")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Databricks multitask pipeline  - Guzzle prepares and submits a set of activities with dependencies to databricks and databricks manages the orchestration of the pipeline"))),(0,r.kt)("br",null),(0,r.kt)("p",null,"Guzzle supports two types of pipeline"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("b",null," Guzzle Pipeline ")," - This is a standard pipeline that is run and orchestrated by Guzzle API"),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("b",null," Databricks multitask pipeline ")," - Guzzle prepares and submits a set of activities with dependencies to databricks and databricks manages the orchestration of the pipeline")),(0,r.kt)("p",null,"In both pipelines, user can configure a static list of activities, and their parameters and override compute configuration but there is no option for a user to configure a list of activities dynamically. to resolve this issue we have added a new feature called dynamic pipeline. For information about the "),(0,r.kt)("h2",{id:"usage"},"Usage"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"We may create a new pipeline in Guzzle by selecting the menu button in the Pipelines section on the left-hand sidebar of Guzzle. We must specify the name of the pipeline according to the action it performs."),(0,r.kt)("li",{parentName:"ul"},"In the Parallel Run section we need to enter a number which determines the number of activities to run simultaneously in the given Pipeline."),(0,r.kt)("li",{parentName:"ul"},"Auto Dependency defines how subsequent activities depend on previous activities, determining the condition of whether to continue executing the next task. If we want to perform a second activity on the data generated in the output of the first activity we must select the Auto Dependency option."),(0,r.kt)("li",{parentName:"ul"},"The Pipeline section can have one or more activities within it. We can mention the different activity names and the parameters associated with each activity. The Parameters are the same as when we fill parameters for individual jobs. They make our pipeline flexible for reuse.")),(0,r.kt)("h2",{id:"steps-to-create-pipeline"},"Steps to create pipeline"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'Pipeline in Guzzle are created by selecting the menu (...) button in the Pipelines section on the left sidebar of Guzzle or "New" button below "Quick Search" which will bring up "Create New Pipeline" diaglog box.  Specify the name the Pipeline in this box.'),(0,r.kt)("li",{parentName:"ul"},'Specify Pipeline configruation as per section "Pipeline Configurations"'),(0,r.kt)("li",{parentName:"ul"},'Add activtiies to the pipeline by either selecting from the  drop-down under column "Acivties" or drag and drop the activties from left sidebard into pipeline. Specify the paremeters for activty by clikcing on cell under the column "Activity Parameter"'),(0,r.kt)("li",{parentName:"ul"},'You can modify Spark settings for a particular activity in pipeline by by clicking "setings". Also Spark and execution realted  settings can be modified globally for the pipeline by clicking on "Setting" button on top right in Pipeline editor. Mored details on how to specify this setting is covered in the section "'),(0,r.kt)("li",{parentName:"ul"},"Dynamic activity can be added to the pipeline "),(0,r.kt)("li",{parentName:"ul"},"A Pipeline dependency graph (Lineage or DAG of pipeline) can also be generated in Guzzle. For this, we need to go to the dependency section and choose the generate option. Guzzle will generate a graph showing the relationship between the different activities in our pipeline.")),(0,r.kt)("img",{width:"1000",src:"/img/docs/how-to-guides/pipeline/pipeline_1.png"}),(0,r.kt)("h2",{id:"pipeline-configurations"},"Pipeline Configurations"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Properties"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Databricks multitask pipeline"),(0,r.kt)("td",{parentName:"tr",align:null},"- In Guzzle, User can configure the pipeline as multitask databricks job. The ability to run multiple tasks as a directed acyclic graph (DAG) ",(0,r.kt)("br",null)," - In multitask job, the user will have the option to configure new job clusters and use them inside the activity. ",(0,r.kt)("br",null)," - Multitask pipeline can run only in databricks compute. ",(0,r.kt)("br",null)," - Guzzle will rely on the status API to fetch multitask pipeline status. in case of status API is down or not reachable, it will retry for a certain interval and after that, It will mark the pipeline as FAILED and NOT_STARTED activity as ABORTED. It will not wait by checking the activity status from the Guzzle repository. ",(0,r.kt)("br",null)," - Multitask job doesn't support external activity - show validation error, if an external activity is configured and the pipeline is multitask pipeline. ",(0,r.kt)("br",null)," - Multitask job will be run from the pipeline thread and wait until it completes its execution.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Auto Dependency"),(0,r.kt)("td",{parentName:"tr",align:null},"- Auto Dependency defines how subsequent activities depend on previous activities, determining the condition of whether to continue executing the next task. If we want to perform a second activity on the data generated in the output of the first activity we must select the Auto Dependency option.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Parallel run"),(0,r.kt)("td",{parentName:"tr",align:null},"- Parallel run number of guzzle activity to run in parallel. For example, We have defined parallel run configuration as 3, Guzzle will run 3 activities in pipeline simultaneously")))),(0,r.kt)("h2",{id:"settings-when-adding-activtiy-to-pipeline"},"Settings when adding Activtiy to pipeline"),(0,r.kt)("p",null,"When clicking on Setttings button for a pipeline we get will be able to update following properties for actvity when exeucting as part of pipeline"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Property"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Override databricks settings"),(0,r.kt)("td",{parentName:"tr",align:null},"This will let you update the databricks compute settsing for a specifci job in pipelie")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Override synapse spark settings"),(0,r.kt)("td",{parentName:"tr",align:null},"Tihs lets you update teh spar")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Retry"),(0,r.kt)("td",{parentName:"tr",align:null},"- There are instances where activity fails in a pipeline due various reasons like network connectivity or other issue and we want to retry pipeline after it failed. ",(0,r.kt)("br",null)," - This feature is only supported when activity is run as part of pipeline. ",(0,r.kt)("br",null)," - Pipeline is to reattempt the same job before moving to next job (in same execution thread assuming parallel jobs are triggered). ",(0,r.kt)("br",null)," -  Retry properties ",(0,r.kt)("br",null)," - Retry status (Failed): If pipeline in Failed state, guzzle will try the job.  ",(0,r.kt)("br",null)," -  Retry interval (seconds) ",(0,r.kt)("br",null)," - Max retry - Number of time Guzzle will retry pieline.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Dependency graph (lineage)"),(0,r.kt)("td",{parentName:"tr",align:null},"- With the help of dependancy graph, we can see ordered flow of pipeline from start to end.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Parameters"),(0,r.kt)("td",{parentName:"tr",align:null},"Guzzle support various parameters that you can override or use in you application. we can set parameter at below hierarchy: ",(0,r.kt)("br",null)," 1. Pipeline Activity ",(0,r.kt)("br",null)," 2. Pipeline level ",(0,r.kt)("br",null)," 3. Runtime Popup ",(0,r.kt)("br",null)," Please check below documentation for ",(0,r.kt)("a",{href:"/docs/how_to_guides/parameters/Parameters"},"Guzzlee parameters")," and ",(0,r.kt)("a",{href:"/docs/how_to_guides/parameters/spark_parameters"},"spark parameter"),".")))),(0,r.kt)("h2",{id:"dependency-graph-lineage"},"Dependency graph (lineage)"),(0,r.kt)("p",null,"With the help of dependancy graph, we can see ordered flow of pipeline from start to end"),(0,r.kt)("img",{width:"1000",src:"/img/docs/how-to-guides/pipeline/pipeline_2.png"}),(0,r.kt)("h2",{id:"pipeline-ui-features"},"Pipeline UI Features"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"We can define list of activites in pipeline. Guzzle support interactive UI feature that we can use to create pipeline."),(0,r.kt)("li",{parentName:"ul"},"Guzzle supports drag and drop to arrange activities inside the pipeline."),(0,r.kt)("li",{parentName:"ul"},"Guzzle allows easy way to pass parameter to individual activities.")),(0,r.kt)("h2",{id:"pipeline-parameters"},"Pipeline parameters"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Parameters are defined at the pipeline level, and can be modified during a pipeline run. Pipeline parameters can be used to control the behavior of a pipeline and its activities, such as by passing in the the path of a file to be processed.")),(0,r.kt)("h3",{id:"how-to-define-a-pipeline-parameter"},"How to define a pipeline parameter"),(0,r.kt)("p",null,"To define a pipeline parameter, follow these steps:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'click on "add parameters" in activity parameters section, '),(0,r.kt)("li",{parentName:"ul"},"here you can define pipeline parameters name and value. whatever parameter you pass in pipeline level are takes highest precedence.",(0,r.kt)("img",{width:"1000",src:"/img/docs/how-to-guides/pipeline/pipeline_parameters.png"}))),(0,r.kt)("h2",{id:"dynamic-activity-support-in-the-pipeline"},"Dynamic Activity support in the pipeline"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Using this feature, User can configure the datastore in the pipeline. he has to specify SQL and configure one or more activities. he has the same options available for which are available to other activities."),(0,r.kt)("li",{parentName:"ul"},"When the pipeline will execute, It will prepare an activity list based on the SQL query results. for each row, it will add configured activities."),(0,r.kt)("li",{parentName:"ul"},"If the query returns zero records it will not add any dynamic activity in the pipeline and continue execution with available activities."),(0,r.kt)("li",{parentName:"ul"},"User can configure multiple datastore and in one datastore multiple activities."),(0,r.kt)("li",{parentName:"ul"},"Users can use a static value or placeholder for the activity name and activity parameter. He can use the column name as a placeholder in #{column_name} format. when it evaluates the pipeline it will be replaced with the actual value of that column."),(0,r.kt)("li",{parentName:"ul"},"If User wants to prevent the dynamic activity creation or parameter passing, he can set null value or null string in the column name."),(0,r.kt)("li",{parentName:"ul"},"When Guzzle will detect a null value or null string in the parameter name or value, it will not pass that parameter to activity."),(0,r.kt)("li",{parentName:"ul"},"If a null value is found for the activity name placeholder, It will not add an activity for that record."),(0,r.kt)("li",{parentName:"ul"},"User can also use Guzzle standard placeholder ${placehodler} inside the datastore sql to pass a value at runtime.")),(0,r.kt)("h3",{id:"supported-datastores"},"Supported datastores:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"JDBC"),(0,r.kt)("li",{parentName:"ol"},"Azure SQL"),(0,r.kt)("li",{parentName:"ol"},"Azure Synapse Analytics"),(0,r.kt)("li",{parentName:"ol"},"Redshift"),(0,r.kt)("li",{parentName:"ol"},"Snowflake")),(0,r.kt)("h3",{id:"placeholder-format"},"Placeholder format"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"#{column_name}"),(0,r.kt)("li",{parentName:"ul"},"Users can not use placeholders with other text.",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"ex. in the activity name, he can not use values like ingestion_#{country}_activity. it will not replace the #{country} placeholder value. It required only placeholders.")))),(0,r.kt)("h2",{id:"pipeline-resume-and-partial-run"},"Pipeline resume and Partial run"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Parameters"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Partial Run guzzle.job_group.partial"),(0,r.kt)("td",{parentName:"tr",align:null},"- A Pipeline can also be configured to Partial load to allow pipeline execution to continue further even if any jobs within pipelines called in the Pipeline fails. ",(0,r.kt)("br",null)," - Please note if Partial Load is not enabled then pipeline execution, then guzzle stops right there as soon as there is any job failure occurs within pipelines.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Resume Pipeline guzzle.job_group.resume"),(0,r.kt)("td",{parentName:"tr",align:null},"- This feature allows to resume the job group or pipeline from where it has failed ",(0,r.kt)("br",null)," - When we resume the pipeline, Guzzle will rerun a FALIED activity or activities which are NOT_STARTED. It will run the FAILED activity from the pipelien where it failed last. This is useful when we need to start execution from failed job, rerunning entire pipeline from beginning is expensive.")))))}c.isMDXComponent=!0}}]);